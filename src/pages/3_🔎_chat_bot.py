from langchain.prompts import PromptTemplate

app_name = "DENSO GPT Expert"

# BOILERPLATE


from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.vectorstores.chroma import Chroma
#from .prompts import prompt

from dotenv import load_dotenv,find_dotenv
import os


####################STAGE 0 LOAD CONFIG ############################
load_dotenv(find_dotenv(),override=True)
CHROMADB_HOST = os.environ.get("CHROMADB_HOST")
CHROMADB_PORT = os.environ.get("CHROMADB_PORT")
OPEN_AI_API_KEY = os.environ.get("OPEN_AI_API_KEY")

llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5, openai_api_key=OPEN_AI_API_KEY)
import streamlit as st
import os
model = HuggingFaceEmbeddings(model_name = "bkai-foundation-models/vietnamese-bi-encoder")
database = Chroma(persist_directory="../chroma_db", embedding_function=model)
st.set_page_config(layout='centered', page_title=f'{app_name}')
ss = st.session_state
if 'debug' not in ss: ss['debug'] = {}

#from DENSO_GPT_Expert.src.Core.model import get_similar_chunks, get_response_from_query
import streamlit as st


API_URL = "https://api-inference.huggingface.co/models/bkai-foundation-models/vietnamese-bi-encoder"
headers = {"Authorization": "Bearer {}".format(os.environ.get("API_TOKEN"))}

st.title("üí¨ DENSO GPT Expert")
st.caption("üöÄ A chatbot powered by SmartABI")

def get_similar_chunks(query, db=database, k=4):
    chunks = db.similarity_search_with_score(query=query, k=k)
    return chunks

def get_response_from_query(query, chunks):
    docs = " ".join([d[0].page_content for d in chunks])

    llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5, openai_api_key=OPEN_AI_API_KEY)

    prompt = PromptTemplate(
        input_variables=["question", "docs"],
        template="""
            B·∫°n l√† ng∆∞·ªùi tr·ª£ l√Ω xu·∫•t s·∫Øc v·ªõi hi·ªÉu bi·∫øt v·ªÅ c√°c t√†i li·ªáu ƒë∆∞·ª£c ƒë∆∞a ra.

            Tr·∫£ l·ªùi c√¢u h·ªèi sau: {question}
            D·ª±a tr√™n t√†i li·ªáu sau: {docs}

            Ch·ªâ s·ª≠ d·ª•ng nh·ªØng th√¥ng tin ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p ƒë·∫øn trong t√†i li·ªáu.

            N·∫øu b·∫°n th·∫•y t√†i li·ªáu kh√¥ng ƒë·ªß th√¥ng tin, h√£y tr·∫£ l·ªùi "T√¥i kh√¥ng c√≥ th√¥ng tin v·ªÅ c√¢u h·ªèi c·ªßa b·∫°n".

            H√£y vi·∫øt l·∫°i c√°c b∆∞·ªõc n·∫øu c√≥ th·ªÉ.

            C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n c·∫ßn ph·∫£i ng·∫Øn g·ªçn v√† s√∫c t√≠ch.
            """,
    )

    chain = LLMChain(llm=llm, prompt=prompt)

    output = chain.run({'question': query, 'docs': docs})
    return output


if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

prompt = st.chat_input("Say something")

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    chunks = get_similar_chunks(query=prompt)
    response = get_response_from_query(query=prompt,chunks=chunks)
    st.session_state.messages.append({"role": "assistant", "content": response})
    st.chat_message("assistant").write(response)

